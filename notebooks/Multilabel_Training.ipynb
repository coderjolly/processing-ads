{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train for 50 epochs. First use only fine tuning on last layer\n",
    "## Then compare with fine-tuning entire network\n",
    "## TODO: \n",
    "## 0. Pre-process images\n",
    "##      Resize images\n",
    "##      Other pre-processing\n",
    "## 1. Get mean and std of dataset\n",
    "## 2. Train on Resnet-34\n",
    "## 3. Train on efficient net b3\n",
    "## 4. Train on mobilenet v3 large\n",
    "## 5. Write a script to plot loss + accuracy graph\n",
    "## 6. Get FLOPs - done\n",
    "## 7. Get num layers - done\n",
    "## 8. Add class weights\n",
    "## 9. Implement gradcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from multilabel.train import train_model\n",
    "from model import initialize_model\n",
    "from utils import set_requires_grad, save_model\n",
    "from multilabel.data import load_data\n",
    "from plotting import plot_data_loader\n",
    "from multilabel.eval import eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "0w24yf-Tj47H"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from numpy.random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.autograd.profiler as tprofiler\n",
    "import torch.utils.data as td\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "O7W8BTtF3BN1"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "\n",
    "# pytorch RNGs\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# numpy RNG\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "21_bts2Wj47M",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "images_dir = \"../data/processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "nr7fQfkuj47u"
   },
   "outputs": [],
   "source": [
    "# Get best num_workers\n",
    "# for i in range(97):\n",
    "#     start = time.time()\n",
    "#     data_loader = load_data(images_dir,\n",
    "#                                                                    batch_size = 96, \n",
    "#                                                                    input_size = 299, \n",
    "#                                                                    norm_arr = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "#                                                                    num_workers = i)\n",
    "#     iter(data_loader['train']).next()[0].shape\n",
    "#     print(f\"{i}: {time.time()-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96wB0P9Gj47u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k-vpcOXE1pmg",
    "outputId": "25752964-a425-490c-d154-0cc8baab3b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Models options: resnet50, resnet34, inceptionv3, vgg16, mobile_net_v3_large, efficient_net_b1, efficient_net_b0, efficient_net_b3.\n",
    "model_name = \"efficient_net_b1\"\n",
    "\n",
    "# Number of classes.\n",
    "num_classes = 20\n",
    "\n",
    "# Batch Size.\n",
    "batch_size = 32\n",
    "\n",
    "# Epochs to train for.\n",
    "num_epochs = 50\n",
    "\n",
    "# Number of workers for data loader.\n",
    "num_workers = 12\n",
    "\n",
    "# Imagenet norm array passed as default value.\n",
    "# norm_arr=([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "# Chest x-ray8 training dataset metrics \n",
    "norm_arr=([0.4955, 0.4955, 0.4955], [0.2892, 0.2892, 0.2892])\n",
    "\n",
    "\n",
    "# Feature extract flag: False - Tune the whole model,\n",
    "#                       True - Update only the reshaped layer parameters.\n",
    "feature_extract = False\n",
    "\n",
    "# Use pretrained flag: None - Use random weights\n",
    "#                      String - Use pretrained weights given by String\n",
    "use_pretrained = None\n",
    "\n",
    "# Initialize the model for this run.\n",
    "model_pyt, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=use_pretrained)\n",
    "\n",
    "# lr start and end points for training.\n",
    "lr_start = 0.01\n",
    "lr_end = 0.001\n",
    "\n",
    "# Print the model we just instantiated\n",
    "#print(model_ft)\n",
    "\n",
    "# Positive class weights.\n",
    "#pos_weight=torch.as_tensor([2., 3., 6., 6., 3., 7., 9.], dtype=torch.float)\n",
    "#pos_weight = pos_weight.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "e9FOwaso3LAc"
   },
   "outputs": [],
   "source": [
    "data_loaders = load_data(images_dir,\n",
    "                         batch_size = batch_size, \n",
    "                         input_size = input_size, \n",
    "                         norm_arr = norm_arr,\n",
    "                         num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Active',\n",
       " 'Alert',\n",
       " 'Amazed',\n",
       " 'Amused',\n",
       " 'Calm',\n",
       " 'Cheerful',\n",
       " 'Confident',\n",
       " 'Conscious',\n",
       " 'Creative',\n",
       " 'Eager',\n",
       " 'Educated',\n",
       " 'Emotional',\n",
       " 'Fashionable',\n",
       " 'Feminine',\n",
       " 'Inspired',\n",
       " 'Loving',\n",
       " 'Manly',\n",
       " 'Persuaded',\n",
       " 'Thrifty',\n",
       " 'Youthful']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loaders['train'].dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_data_loader(data_loaders['train'], (2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "vacZgHSCj47u",
    "outputId": "35a65cef-1d6d-4657-ff01-be15854ca24b"
   },
   "source": [
    "plot_data_loader(data_loader['train'], (2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRIOYWbV1cnS"
   },
   "source": [
    "plot_data_loader(data_loader['test'], (2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znBg5tkd1dXF"
   },
   "source": [
    "plot_data_loader(data_loader['val'], (2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "0rj7Qeg41wLm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t features.0.0.weight\n",
      "\t features.0.1.weight\n",
      "\t features.0.1.bias\n",
      "\t features.1.0.block.0.0.weight\n",
      "\t features.1.0.block.0.1.weight\n",
      "\t features.1.0.block.0.1.bias\n",
      "\t features.1.0.block.1.fc1.weight\n",
      "\t features.1.0.block.1.fc1.bias\n",
      "\t features.1.0.block.1.fc2.weight\n",
      "\t features.1.0.block.1.fc2.bias\n",
      "\t features.1.0.block.2.0.weight\n",
      "\t features.1.0.block.2.1.weight\n",
      "\t features.1.0.block.2.1.bias\n",
      "\t features.1.1.block.0.0.weight\n",
      "\t features.1.1.block.0.1.weight\n",
      "\t features.1.1.block.0.1.bias\n",
      "\t features.1.1.block.1.fc1.weight\n",
      "\t features.1.1.block.1.fc1.bias\n",
      "\t features.1.1.block.1.fc2.weight\n",
      "\t features.1.1.block.1.fc2.bias\n",
      "\t features.1.1.block.2.0.weight\n",
      "\t features.1.1.block.2.1.weight\n",
      "\t features.1.1.block.2.1.bias\n",
      "\t features.2.0.block.0.0.weight\n",
      "\t features.2.0.block.0.1.weight\n",
      "\t features.2.0.block.0.1.bias\n",
      "\t features.2.0.block.1.0.weight\n",
      "\t features.2.0.block.1.1.weight\n",
      "\t features.2.0.block.1.1.bias\n",
      "\t features.2.0.block.2.fc1.weight\n",
      "\t features.2.0.block.2.fc1.bias\n",
      "\t features.2.0.block.2.fc2.weight\n",
      "\t features.2.0.block.2.fc2.bias\n",
      "\t features.2.0.block.3.0.weight\n",
      "\t features.2.0.block.3.1.weight\n",
      "\t features.2.0.block.3.1.bias\n",
      "\t features.2.1.block.0.0.weight\n",
      "\t features.2.1.block.0.1.weight\n",
      "\t features.2.1.block.0.1.bias\n",
      "\t features.2.1.block.1.0.weight\n",
      "\t features.2.1.block.1.1.weight\n",
      "\t features.2.1.block.1.1.bias\n",
      "\t features.2.1.block.2.fc1.weight\n",
      "\t features.2.1.block.2.fc1.bias\n",
      "\t features.2.1.block.2.fc2.weight\n",
      "\t features.2.1.block.2.fc2.bias\n",
      "\t features.2.1.block.3.0.weight\n",
      "\t features.2.1.block.3.1.weight\n",
      "\t features.2.1.block.3.1.bias\n",
      "\t features.2.2.block.0.0.weight\n",
      "\t features.2.2.block.0.1.weight\n",
      "\t features.2.2.block.0.1.bias\n",
      "\t features.2.2.block.1.0.weight\n",
      "\t features.2.2.block.1.1.weight\n",
      "\t features.2.2.block.1.1.bias\n",
      "\t features.2.2.block.2.fc1.weight\n",
      "\t features.2.2.block.2.fc1.bias\n",
      "\t features.2.2.block.2.fc2.weight\n",
      "\t features.2.2.block.2.fc2.bias\n",
      "\t features.2.2.block.3.0.weight\n",
      "\t features.2.2.block.3.1.weight\n",
      "\t features.2.2.block.3.1.bias\n",
      "\t features.3.0.block.0.0.weight\n",
      "\t features.3.0.block.0.1.weight\n",
      "\t features.3.0.block.0.1.bias\n",
      "\t features.3.0.block.1.0.weight\n",
      "\t features.3.0.block.1.1.weight\n",
      "\t features.3.0.block.1.1.bias\n",
      "\t features.3.0.block.2.fc1.weight\n",
      "\t features.3.0.block.2.fc1.bias\n",
      "\t features.3.0.block.2.fc2.weight\n",
      "\t features.3.0.block.2.fc2.bias\n",
      "\t features.3.0.block.3.0.weight\n",
      "\t features.3.0.block.3.1.weight\n",
      "\t features.3.0.block.3.1.bias\n",
      "\t features.3.1.block.0.0.weight\n",
      "\t features.3.1.block.0.1.weight\n",
      "\t features.3.1.block.0.1.bias\n",
      "\t features.3.1.block.1.0.weight\n",
      "\t features.3.1.block.1.1.weight\n",
      "\t features.3.1.block.1.1.bias\n",
      "\t features.3.1.block.2.fc1.weight\n",
      "\t features.3.1.block.2.fc1.bias\n",
      "\t features.3.1.block.2.fc2.weight\n",
      "\t features.3.1.block.2.fc2.bias\n",
      "\t features.3.1.block.3.0.weight\n",
      "\t features.3.1.block.3.1.weight\n",
      "\t features.3.1.block.3.1.bias\n",
      "\t features.3.2.block.0.0.weight\n",
      "\t features.3.2.block.0.1.weight\n",
      "\t features.3.2.block.0.1.bias\n",
      "\t features.3.2.block.1.0.weight\n",
      "\t features.3.2.block.1.1.weight\n",
      "\t features.3.2.block.1.1.bias\n",
      "\t features.3.2.block.2.fc1.weight\n",
      "\t features.3.2.block.2.fc1.bias\n",
      "\t features.3.2.block.2.fc2.weight\n",
      "\t features.3.2.block.2.fc2.bias\n",
      "\t features.3.2.block.3.0.weight\n",
      "\t features.3.2.block.3.1.weight\n",
      "\t features.3.2.block.3.1.bias\n",
      "\t features.4.0.block.0.0.weight\n",
      "\t features.4.0.block.0.1.weight\n",
      "\t features.4.0.block.0.1.bias\n",
      "\t features.4.0.block.1.0.weight\n",
      "\t features.4.0.block.1.1.weight\n",
      "\t features.4.0.block.1.1.bias\n",
      "\t features.4.0.block.2.fc1.weight\n",
      "\t features.4.0.block.2.fc1.bias\n",
      "\t features.4.0.block.2.fc2.weight\n",
      "\t features.4.0.block.2.fc2.bias\n",
      "\t features.4.0.block.3.0.weight\n",
      "\t features.4.0.block.3.1.weight\n",
      "\t features.4.0.block.3.1.bias\n",
      "\t features.4.1.block.0.0.weight\n",
      "\t features.4.1.block.0.1.weight\n",
      "\t features.4.1.block.0.1.bias\n",
      "\t features.4.1.block.1.0.weight\n",
      "\t features.4.1.block.1.1.weight\n",
      "\t features.4.1.block.1.1.bias\n",
      "\t features.4.1.block.2.fc1.weight\n",
      "\t features.4.1.block.2.fc1.bias\n",
      "\t features.4.1.block.2.fc2.weight\n",
      "\t features.4.1.block.2.fc2.bias\n",
      "\t features.4.1.block.3.0.weight\n",
      "\t features.4.1.block.3.1.weight\n",
      "\t features.4.1.block.3.1.bias\n",
      "\t features.4.2.block.0.0.weight\n",
      "\t features.4.2.block.0.1.weight\n",
      "\t features.4.2.block.0.1.bias\n",
      "\t features.4.2.block.1.0.weight\n",
      "\t features.4.2.block.1.1.weight\n",
      "\t features.4.2.block.1.1.bias\n",
      "\t features.4.2.block.2.fc1.weight\n",
      "\t features.4.2.block.2.fc1.bias\n",
      "\t features.4.2.block.2.fc2.weight\n",
      "\t features.4.2.block.2.fc2.bias\n",
      "\t features.4.2.block.3.0.weight\n",
      "\t features.4.2.block.3.1.weight\n",
      "\t features.4.2.block.3.1.bias\n",
      "\t features.4.3.block.0.0.weight\n",
      "\t features.4.3.block.0.1.weight\n",
      "\t features.4.3.block.0.1.bias\n",
      "\t features.4.3.block.1.0.weight\n",
      "\t features.4.3.block.1.1.weight\n",
      "\t features.4.3.block.1.1.bias\n",
      "\t features.4.3.block.2.fc1.weight\n",
      "\t features.4.3.block.2.fc1.bias\n",
      "\t features.4.3.block.2.fc2.weight\n",
      "\t features.4.3.block.2.fc2.bias\n",
      "\t features.4.3.block.3.0.weight\n",
      "\t features.4.3.block.3.1.weight\n",
      "\t features.4.3.block.3.1.bias\n",
      "\t features.5.0.block.0.0.weight\n",
      "\t features.5.0.block.0.1.weight\n",
      "\t features.5.0.block.0.1.bias\n",
      "\t features.5.0.block.1.0.weight\n",
      "\t features.5.0.block.1.1.weight\n",
      "\t features.5.0.block.1.1.bias\n",
      "\t features.5.0.block.2.fc1.weight\n",
      "\t features.5.0.block.2.fc1.bias\n",
      "\t features.5.0.block.2.fc2.weight\n",
      "\t features.5.0.block.2.fc2.bias\n",
      "\t features.5.0.block.3.0.weight\n",
      "\t features.5.0.block.3.1.weight\n",
      "\t features.5.0.block.3.1.bias\n",
      "\t features.5.1.block.0.0.weight\n",
      "\t features.5.1.block.0.1.weight\n",
      "\t features.5.1.block.0.1.bias\n",
      "\t features.5.1.block.1.0.weight\n",
      "\t features.5.1.block.1.1.weight\n",
      "\t features.5.1.block.1.1.bias\n",
      "\t features.5.1.block.2.fc1.weight\n",
      "\t features.5.1.block.2.fc1.bias\n",
      "\t features.5.1.block.2.fc2.weight\n",
      "\t features.5.1.block.2.fc2.bias\n",
      "\t features.5.1.block.3.0.weight\n",
      "\t features.5.1.block.3.1.weight\n",
      "\t features.5.1.block.3.1.bias\n",
      "\t features.5.2.block.0.0.weight\n",
      "\t features.5.2.block.0.1.weight\n",
      "\t features.5.2.block.0.1.bias\n",
      "\t features.5.2.block.1.0.weight\n",
      "\t features.5.2.block.1.1.weight\n",
      "\t features.5.2.block.1.1.bias\n",
      "\t features.5.2.block.2.fc1.weight\n",
      "\t features.5.2.block.2.fc1.bias\n",
      "\t features.5.2.block.2.fc2.weight\n",
      "\t features.5.2.block.2.fc2.bias\n",
      "\t features.5.2.block.3.0.weight\n",
      "\t features.5.2.block.3.1.weight\n",
      "\t features.5.2.block.3.1.bias\n",
      "\t features.5.3.block.0.0.weight\n",
      "\t features.5.3.block.0.1.weight\n",
      "\t features.5.3.block.0.1.bias\n",
      "\t features.5.3.block.1.0.weight\n",
      "\t features.5.3.block.1.1.weight\n",
      "\t features.5.3.block.1.1.bias\n",
      "\t features.5.3.block.2.fc1.weight\n",
      "\t features.5.3.block.2.fc1.bias\n",
      "\t features.5.3.block.2.fc2.weight\n",
      "\t features.5.3.block.2.fc2.bias\n",
      "\t features.5.3.block.3.0.weight\n",
      "\t features.5.3.block.3.1.weight\n",
      "\t features.5.3.block.3.1.bias\n",
      "\t features.6.0.block.0.0.weight\n",
      "\t features.6.0.block.0.1.weight\n",
      "\t features.6.0.block.0.1.bias\n",
      "\t features.6.0.block.1.0.weight\n",
      "\t features.6.0.block.1.1.weight\n",
      "\t features.6.0.block.1.1.bias\n",
      "\t features.6.0.block.2.fc1.weight\n",
      "\t features.6.0.block.2.fc1.bias\n",
      "\t features.6.0.block.2.fc2.weight\n",
      "\t features.6.0.block.2.fc2.bias\n",
      "\t features.6.0.block.3.0.weight\n",
      "\t features.6.0.block.3.1.weight\n",
      "\t features.6.0.block.3.1.bias\n",
      "\t features.6.1.block.0.0.weight\n",
      "\t features.6.1.block.0.1.weight\n",
      "\t features.6.1.block.0.1.bias\n",
      "\t features.6.1.block.1.0.weight\n",
      "\t features.6.1.block.1.1.weight\n",
      "\t features.6.1.block.1.1.bias\n",
      "\t features.6.1.block.2.fc1.weight\n",
      "\t features.6.1.block.2.fc1.bias\n",
      "\t features.6.1.block.2.fc2.weight\n",
      "\t features.6.1.block.2.fc2.bias\n",
      "\t features.6.1.block.3.0.weight\n",
      "\t features.6.1.block.3.1.weight\n",
      "\t features.6.1.block.3.1.bias\n",
      "\t features.6.2.block.0.0.weight\n",
      "\t features.6.2.block.0.1.weight\n",
      "\t features.6.2.block.0.1.bias\n",
      "\t features.6.2.block.1.0.weight\n",
      "\t features.6.2.block.1.1.weight\n",
      "\t features.6.2.block.1.1.bias\n",
      "\t features.6.2.block.2.fc1.weight\n",
      "\t features.6.2.block.2.fc1.bias\n",
      "\t features.6.2.block.2.fc2.weight\n",
      "\t features.6.2.block.2.fc2.bias\n",
      "\t features.6.2.block.3.0.weight\n",
      "\t features.6.2.block.3.1.weight\n",
      "\t features.6.2.block.3.1.bias\n",
      "\t features.6.3.block.0.0.weight\n",
      "\t features.6.3.block.0.1.weight\n",
      "\t features.6.3.block.0.1.bias\n",
      "\t features.6.3.block.1.0.weight\n",
      "\t features.6.3.block.1.1.weight\n",
      "\t features.6.3.block.1.1.bias\n",
      "\t features.6.3.block.2.fc1.weight\n",
      "\t features.6.3.block.2.fc1.bias\n",
      "\t features.6.3.block.2.fc2.weight\n",
      "\t features.6.3.block.2.fc2.bias\n",
      "\t features.6.3.block.3.0.weight\n",
      "\t features.6.3.block.3.1.weight\n",
      "\t features.6.3.block.3.1.bias\n",
      "\t features.6.4.block.0.0.weight\n",
      "\t features.6.4.block.0.1.weight\n",
      "\t features.6.4.block.0.1.bias\n",
      "\t features.6.4.block.1.0.weight\n",
      "\t features.6.4.block.1.1.weight\n",
      "\t features.6.4.block.1.1.bias\n",
      "\t features.6.4.block.2.fc1.weight\n",
      "\t features.6.4.block.2.fc1.bias\n",
      "\t features.6.4.block.2.fc2.weight\n",
      "\t features.6.4.block.2.fc2.bias\n",
      "\t features.6.4.block.3.0.weight\n",
      "\t features.6.4.block.3.1.weight\n",
      "\t features.6.4.block.3.1.bias\n",
      "\t features.7.0.block.0.0.weight\n",
      "\t features.7.0.block.0.1.weight\n",
      "\t features.7.0.block.0.1.bias\n",
      "\t features.7.0.block.1.0.weight\n",
      "\t features.7.0.block.1.1.weight\n",
      "\t features.7.0.block.1.1.bias\n",
      "\t features.7.0.block.2.fc1.weight\n",
      "\t features.7.0.block.2.fc1.bias\n",
      "\t features.7.0.block.2.fc2.weight\n",
      "\t features.7.0.block.2.fc2.bias\n",
      "\t features.7.0.block.3.0.weight\n",
      "\t features.7.0.block.3.1.weight\n",
      "\t features.7.0.block.3.1.bias\n",
      "\t features.7.1.block.0.0.weight\n",
      "\t features.7.1.block.0.1.weight\n",
      "\t features.7.1.block.0.1.bias\n",
      "\t features.7.1.block.1.0.weight\n",
      "\t features.7.1.block.1.1.weight\n",
      "\t features.7.1.block.1.1.bias\n",
      "\t features.7.1.block.2.fc1.weight\n",
      "\t features.7.1.block.2.fc1.bias\n",
      "\t features.7.1.block.2.fc2.weight\n",
      "\t features.7.1.block.2.fc2.bias\n",
      "\t features.7.1.block.3.0.weight\n",
      "\t features.7.1.block.3.1.weight\n",
      "\t features.7.1.block.3.1.bias\n",
      "\t features.8.0.weight\n",
      "\t features.8.1.weight\n",
      "\t features.8.1.bias\n",
      "\t classifier.1.weight\n",
      "\t classifier.1.bias\n"
     ]
    }
   ],
   "source": [
    "# Send model to GPU\n",
    "model_pyt = model_pyt.to(device)\n",
    "\n",
    "# Find parameters to be updated in this run.\n",
    "# parameters with requires_grad = True.\n",
    "params_to_update = model_pyt.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_pyt.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_pyt.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "A4B1-pfYm0Ea"
   },
   "outputs": [],
   "source": [
    "# 17 min 1 epoch - 128 batch size - inception\n",
    "# Efficientnet b0 - batch 96 - epoch 50 - num_workers 2 - flip, auto cont, sharp - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8wBR8vcG2jcd",
    "outputId": "7d01aa07-d235-4cb6-dfaf-53ce0c5a577d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 5.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\ro_chopr\\.conda\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"c:\\Users\\ro_chopr\\.conda\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\ro_chopr\\.conda\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\ro_chopr\\Documents\\processing-ads\\notebooks\\../src\\multilabel\\loader.py\", line 22, in __getitem__\n    image = self.transform(image)\n  File \"c:\\Users\\ro_chopr\\.conda\\envs\\dl\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 95, in __call__\n    img = t(img)\n  File \"c:\\Users\\ro_chopr\\.conda\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"c:\\Users\\ro_chopr\\.conda\\envs\\dl\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 277, in forward\n    return F.normalize(tensor, self.mean, self.std, self.inplace)\n  File \"c:\\Users\\ro_chopr\\.conda\\envs\\dl\\lib\\site-packages\\torchvision\\transforms\\functional.py\", line 363, in normalize\n    return F_t.normalize(tensor, mean=mean, std=std, inplace=inplace)\n  File \"c:\\Users\\ro_chopr\\.conda\\envs\\dl\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py\", line 928, in normalize\n    return tensor.sub_(mean).div_(std)\nRuntimeError: output with shape [1, 255, 255] doesn't match the broadcast shape [3, 255, 255]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [54], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mBCEWithLogitsLoss()\n\u001b[0;32m     13\u001b[0m \u001b[39m# Train and evaluate\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m model_pyt, prof, val_history, train_history \u001b[39m=\u001b[39m train_model(device, model_pyt, data_loaders, \n\u001b[0;32m     15\u001b[0m                                                             optimizer, scheduler,\n\u001b[0;32m     16\u001b[0m                                                             criterion, \n\u001b[0;32m     17\u001b[0m                                                             num_epochs\u001b[39m=\u001b[39;49mnum_epochs,\n\u001b[0;32m     18\u001b[0m                                                             num_classes\u001b[39m=\u001b[39;49mnum_classes,\n\u001b[0;32m     19\u001b[0m                                                             is_inception\u001b[39m=\u001b[39;49m(model_name\u001b[39m==\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39minceptionv3\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m     20\u001b[0m                                                             profiler\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\ro_chopr\\Documents\\processing-ads\\notebooks\\../src\\multilabel\\train.py:52\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(device, model, dataloaders, optimizer, scheduler, criterion, num_epochs, num_classes, is_inception, profiler, threshold)\u001b[0m\n\u001b[0;32m     49\u001b[0m outputs_batch \u001b[39m=\u001b[39m []\n\u001b[0;32m     51\u001b[0m \u001b[39m# Iterate over data.\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m dataloaders[phase]:\n\u001b[0;32m     53\u001b[0m     i\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m     54\u001b[0m     inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\ro_chopr\\.conda\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\ro_chopr\\.conda\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1344\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1345\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1346\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[1;32mc:\\Users\\ro_chopr\\.conda\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[0;32m   1371\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1372\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[0;32m   1373\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\ro_chopr\\.conda\\envs\\dl\\lib\\site-packages\\torch\\_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 5.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\ro_chopr\\.conda\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"c:\\Users\\ro_chopr\\.conda\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\ro_chopr\\.conda\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\Users\\ro_chopr\\Documents\\processing-ads\\notebooks\\../src\\multilabel\\loader.py\", line 22, in __getitem__\n    image = self.transform(image)\n  File \"c:\\Users\\ro_chopr\\.conda\\envs\\dl\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 95, in __call__\n    img = t(img)\n  File \"c:\\Users\\ro_chopr\\.conda\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"c:\\Users\\ro_chopr\\.conda\\envs\\dl\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 277, in forward\n    return F.normalize(tensor, self.mean, self.std, self.inplace)\n  File \"c:\\Users\\ro_chopr\\.conda\\envs\\dl\\lib\\site-packages\\torchvision\\transforms\\functional.py\", line 363, in normalize\n    return F_t.normalize(tensor, mean=mean, std=std, inplace=inplace)\n  File \"c:\\Users\\ro_chopr\\.conda\\envs\\dl\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py\", line 928, in normalize\n    return tensor.sub_(mean).div_(std)\nRuntimeError: output with shape [1, 255, 255] doesn't match the broadcast shape [3, 255, 255]\n"
     ]
    }
   ],
   "source": [
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.Adam(params_to_update, lr=lr_start)\n",
    "\n",
    "# Learning rate scheduler.\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=lr_end,\n",
    "                                                 last_epoch=-1)\n",
    "\n",
    "\n",
    "# Setup the loss fxn\n",
    "# pos_weight=pos_weight\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_pyt, prof, val_history, train_history = train_model(device, model_pyt, data_loaders, \n",
    "                                                            optimizer, scheduler,\n",
    "                                                            criterion, \n",
    "                                                            num_epochs=num_epochs,\n",
    "                                                            num_classes=num_classes,\n",
    "                                                            is_inception=(model_name==\"inceptionv3\"),\n",
    "                                                            profiler=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'chest_xray_8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zM-dorQBJAZb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(model_pyt.state_dict(), '../../models/', \n",
    "           f'{model_name}_{dataset_name}_{num_epochs}_{batch_size}_{lr_start}_{lr_end}_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = eval_model(device=device, model=model_pyt, test_loader=data_loaders['test'], is_inception=(model_name==\"inceptionv3\"), num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.77413485079071\n",
      "f1: 0.4459599730627861\n",
      "cm: [[[2708  565]\n",
      "  [ 326  719]]\n",
      "\n",
      " [[2864  409]\n",
      "  [ 403  642]]\n",
      "\n",
      " [[2946  813]\n",
      "  [ 228  331]]\n",
      "\n",
      " [[2982  681]\n",
      "  [ 387  268]]\n",
      "\n",
      " [[2695  576]\n",
      "  [ 552  495]]\n",
      "\n",
      " [[3250  623]\n",
      "  [ 228  217]]\n",
      "\n",
      " [[2986  866]\n",
      "  [ 170  296]]]\n",
      "outputs: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]]\n",
      "targets: [[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "for i, v in eval_metrics.items():\n",
    "    print(f\"{i}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../../models/val_history_{model_name}_{dataset_name}_{num_epochs}_{batch_size}_{lr_start}_{lr_end}_.pickle', 'wb') as handle:\n",
    "    pickle.dump(val_history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(f'../../models/train_history_{model_name}_{dataset_name}_{num_epochs}_{batch_size}_{lr_start}_{lr_end}_.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(f'../../models/eval_metrics_{model_name}_{dataset_name}_{num_epochs}_{batch_size}_{lr_start}_{lr_end}_.pickle', 'wb') as handle:\n",
    "    pickle.dump(eval_metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#with open('filename.pickle', 'rb') as handle:\n",
    "#    b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvFygat_aiDN"
   },
   "outputs": [],
   "source": [
    "#print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EQ6hb3iO2mXv"
   },
   "outputs": [],
   "source": [
    "# Plot the training curves of validation accuracy vs. number\n",
    "#  of training epochs for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "# vhist = []\n",
    "# vhist = [h.cpu().numpy() for h in val_acc_history]\n",
    "# thist = []\n",
    "# thist = [h.cpu().numpy() for h in train_acc_history]\n",
    "\n",
    "# plt.title(\"Accuracy vs. Number of Training Epochs\")\n",
    "# plt.xlabel(\"Training Epochs\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# #plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "# plt.plot(range(1,num_epochs+1),vhist,label=\"Validation\")\n",
    "# plt.plot(range(1,num_epochs+1),thist,label=\"Training\")\n",
    "# plt.ylim((0,1.))\n",
    "# plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aXpHASjTUE_Q"
   },
   "outputs": [],
   "source": [
    "# Plot the training curves of validation accuracy vs. number\n",
    "#  of training epochs for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "# vhist = []\n",
    "# vhist = [h for h in val_loss_history]\n",
    "# thist = []\n",
    "# thist = [h for h in train_loss_history]\n",
    "\n",
    "# plt.title(\"Loss vs. Number of Training Epochs\")\n",
    "# plt.xlabel(\"Training Epochs\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# #plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "# plt.plot(range(1,num_epochs+1),vhist,label=\"Validation\")\n",
    "# plt.plot(range(1,num_epochs+1),thist,label=\"Training\")\n",
    "# plt.ylim((0,1.))\n",
    "# plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8663be1127523d7121742bbf948a8b1c8dd9a63c15e224e5108ff87b090569d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
